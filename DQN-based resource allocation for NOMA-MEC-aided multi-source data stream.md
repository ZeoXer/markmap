# 文件結構

## 1. 介紹
- 無線通訊技術進步
- 移動設備數量增加導致數據處理需求增加
- 本地計算能力不足，雲端計算過載問題
- 移動邊緣計算（MEC）應運而生
- 非正交多址技術（NOMA）介紹

## 2. 系統模型
- NOMA輔助MEC網絡結構
- 數據流卸載模型
- 本地計算模型
- CAP計算模型
- 問題公式化

## 3. 基於DQN的卸載策略
- 馬爾可夫決策過程（MDP）
- 深度Q網絡（DQN）優化方案
- 獎勵設計
- 實驗設計與處理

## 4. 實驗結果
- 實驗設計
- 模擬結果

## 5. 結論

# 詳細筆記

## 1. 介紹
- 📈 **無線通訊技術進步**：無線技術的迅速發展使得移動設備數量急劇增加。
- 💻 **本地計算問題**：設備的本地計算能力有限，難以應對龐大的數據流。
- ☁️ **雲端計算過載**：過多數據卸載至雲端，導致通信延遲和系統性能下降。
- 🖥️ **MEC引入**：MEC在邊緣計算數據流，分散計算負載，減少延遲。
- 🔗 **NOMA技術**：使用NOMA技術減少MEC網絡中的能耗和延遲，提升傳輸速率。

## 2. 系統模型
- 📊 **NOMA輔助MEC網絡結構**：多源頭和一個計算訪問點（CAP）的結構。
- 🔄 **數據流卸載模型**：
  - 多個源頭通過NOMA技術將部分數據卸載到CAP。
  - 傳輸速率公式：$r_s = B \log_2(1 + \frac{P_s|h_s|^2}{\sum_{n=1}^{s-1}P_n|h_n|^2 + \sigma^2})$
  - 傳輸延遲公式：$t_s = \frac{\beta_s q_s}{r_s}$
- 🖥️ **本地計算模型**：
  - 本地計算延遲公式：$t_{s,local} = \frac{(1-\beta_s)c_s}{f_s}$
  - 本地計算能耗公式：$E_2 = \sum_{s=1}^S t_{s,local} P_{s,local}$
- 📡 **CAP計算模型**：
  - CAP計算延遲公式：$t_{s,MEC} = \frac{\beta_s c_s}{F_s}$
  - CAP計算能耗公式：$E_3 = \sum_{s=1}^S t_{s,MEC} P_{s,MEC}$
- 🧮 **問題公式化**：
  - 系統總延遲：$T_{total} = \max \{T_2, T_1 + T_3\}$
  - 系統總能耗：$E_{total} = E_1 + E_2 + E_3$
  - 系統成本函數：$\Phi_s = \mu T_{total} + (1-\mu)E_{total}$

## 3. 基於DQN的卸載策略
- 🧠 **MDP模型**：將卸載決策建模為馬爾可夫決策過程。
  - **狀態（State）**：包含系統的當前信息，如計算資源使用情況、通道狀況、當前數據量、本地設備和CAP的計算能力。
  - **動作（Action）**：表示採取的行動，如決定數據卸載比例 $\beta_s$ 和傳輸功率分配 $P_s$。
  - **獎勵（Reward）**：目的是最小化系統成本，包括延遲和能耗，設計為 $r = - (\mu T_{total} + (1-\mu)E_{total})$。
  - **轉移概率（Transition Probability）**：表示從當前狀態 $s$ 採取動作 $a$ 後轉移到下一狀態 $s'$ 的概率。
- 💡 **DQN優化**：使用深度Q學習算法進行卸載比率和傳輸功率分配的優化。
  - **Q函數設計**：Q函數 $Q(s, a)$ 表示在狀態 $s$ 下採取動作 $a$ 所得到的預期回報。
  - **神經網絡結構**：深度Q網絡通常包含多層全連接層，輸入為狀態 $s$，輸出為所有可能動作的Q值。
  - **損失函數**：衡量Q值的預測誤差，常用均方誤差：$L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} [(r + \gamma \max_{a'} Q(s', a'; \theta^{-}) - Q(s, a; \theta))^2]$。
- 🏆 **獎勵設計**：最小化系統成本作為獎勵函數。
- 🔬 **實驗設計與處理**：
  - **實驗設計**：設置不同的環境參數，如不同數量的源頭節點、計算資源配置、通道狀況。
  - **實驗步驟**：
    1. 初始化DQN的參數和神經網絡，初始化環境參數和狀態。
    2. 在每個時間步長觀察當前狀態 $s$。
    3. 根據DQN的Q值選擇動作 $a$（通常使用ε-greedy策略）。
    4. 執行選擇的動作，觀察環境反饋（即下一狀態 $s'$ 和獎勵 $r$）。
    5. 使用經驗回放技術從記憶庫中隨機抽取樣本進行訓練，計算損失函數並反向傳播更新網絡參數。
    6. 重複過程直到達到預定的訓練次數或收斂條件。

## 4. 實驗結果
- 🧪 **實驗設計**：比較不同策略下的系統性能。
- 📉 **模擬結果**：
  - 基於DQN的策略相較於其他方法顯著降低了總系統成本。
  - 當源頭數量為5時，系統成本降低約15%。

## 5. 結論
- ✅ **總結**：基於DQN的卸載策略在降低系統成本方面具有顯著優勢，適用於動態網絡環境。
