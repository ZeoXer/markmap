# 🚗 Com-DDPG 研究筆記

## 文獻大綱

### 相關工作

- 單用戶與多用戶計算卸載比較
  - 單用戶: 主要目標是減少移動終端設備的能耗或平衡能耗與延遲
  - 多用戶: 著重於提高多用戶計算卸載的效率與性能

### 系統架構

- 描述了 MEC 系統中的架構
- 強調系統內部的狀態預測和多智能體間的通信

### 計算卸載方法

- 提出了基於 Com-DDPG 算法的計算卸載方法
  - 使用 BRNN 結構進行多智能體間的信息共享
  - LSTM 模塊用於環境狀態預測

### 實驗與結果分析

- 實驗設置

  - 使用來自阿里巴巴集群數據的日志數據進行模擬
  - 訓練集與測試集的選擇標準

- 主要實驗結果
  - **損失函數實驗**
    - 定義: $L = |R_{algorithm} - R_{datasets}|$
    - Com-DDPG 相比其他算法有更小的損失函數值
  - **最大完成時間實驗**
    - 定義: 任務完成時間減去任務提交時間
    - Com-DDPG 能顯著減少最大完成時間&#8203;:citation[oaicite:1]{index=1}&#8203;
  - **服務時間實驗**
    - 比較了不同算法下每個服務器的運行時間
    - Com-DDPG 在服務器選擇上更為均衡&#8203;:citation[oaicite:0]{index=0}&#8203;

### 結論與未來研究方向

- Com-DDPG 算法在多用戶計算卸載中展現出較好的性能和穩定性
- 提出了未來研究可以進一步改進的方法與方向

---

## 💡 知識點總結

### 計算卸載的挑戰 🚀

- 高效的卸載策略
  - 單用戶卸載: 最小化能源消耗或平衡延遲
  - 多用戶卸載: 涉及資源分配和任務調度
- 相關研究
  - 基於 Q-learning 的卸載和資源分配策略
  - 基於博弈論和 Lyapunov 最優化理論的策略
  - 基於深度強化學習的多智能體卸載方案

### 多智能體強化學習 🔄

- 深度 Q 網路 (DQN) 和深度遞歸 Q 網路 (DRQN)
  - DQN: 對環境做出決策
  - DRQN: 結合 LSTM 處理時間序列數據
- 複合深度確定性策略梯度 (Com-DDPG) 算法
  - 結合 LSTM 模塊的多智能體協同通信
  - 取得環境的更多信息以優化卸載決策

### 性能指標 📊

- 最大完成時間
  - 隨著訓練過程中任務塊數量的增加而減少
  - Com-DDPG 算法比 DQN 和 DRQN 算法更穩定
- 服務時間
  - Com-DDPG 支持多智能體通信，選擇目標服務器更具優化

---

## 🧪 研究方法

### 系統架構設計 🏗️

- 計算卸載系統架構
  - 多接入邊緣計算 (MEC) 環境中的任務卸載策略
- 動作空間
  - 任務對應計算設備的映射
  - 有效動作空間的預處理

### 數學推導過程 📐

- 獎勵函數 (Reward Function) 💰
  - 影響因素: 能耗、平均延遲、任務優先級、設備負載
  - 獎勵函數公式: $R = -(Ω_1Z_{en}(t_0) + Ω_2(Z_{la}(t_0) + P) + Ω_3Z_{ls}(t_0))$
- 平均延遲 (Average Latency)
  - 數據傳輸時間 $LA_{trans}$ 和計算時間 $LA_{comp}$
  - 平均延遲公式: $LA_{avg}(t_0) = \frac{\sum_{j=1}^{AN} \sum_{i=1}^{TN_j} (LA_{trans_i}(t_0) + LA_{comp_i}(t_0))}{AN}$

### 實驗設置 🧪

- 最大完成時間實驗
  - 比較不同強化學習算法的最大完成時間
  - 通過箱線圖展示結果
- 服務時間實驗
  - 模擬 100 個任務，監測各服務器的運行時間
  - 使用熱圖展示不同算法的服務時間分佈

### 實驗結果與分析 📈

- 損失函數實驗
  - 損失函數值越小，結果越好
  - 比較 DQN, DRQN 和 Com-DDPG 算法的損失函數得分
- 計算卸載性能實驗
  - Com-DDPG 算法在穩定性和魯棒性方面優於其他算法

---

### LSTM 和 Com-DDPG 優化

#### LSTM 在 Com-DDPG 中的應用

- **內部狀態預測結構**
  - 包含卷積神經網絡 (CNN)、注意力網絡和長短期記憶 (LSTM) 網絡。
  - CNN 提取環境特徵，注意力網絡輸出上下文向量，LSTM 網絡生成新的隱藏狀態和記憶狀態。

#### Com-DDPG 的優化設計

- **多智能體通信網絡**
  - 結合 DDPG 和多連接網絡，使智能體在執行動作前互相通信。
  - 多智能體策略網絡和多智能體 Q 網絡都基於雙向循環神經網絡 (BRNN)。

---

## 🔚 結論

- Com-DDPG 算法在 MEC 中展現出優異的性能和穩定性。
- 未來研究方向包括進一步優化算法和擴展應用場景。
